{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [***Getting started with text preprocessing***](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing) by SRK on Kaggle.\n",
    "More information on the [***dataset***](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter).\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important. \n",
    "\n",
    "Objective of this kernel is to understand the various text preprocessing steps with code examples. \n",
    "\n",
    "Some of the common text preprocessing / cleaning steps are:\n",
    "* Lower casing\n",
    "* Removal of Punctuations\n",
    "* Removal of Stopwords\n",
    "* Removal of Frequent words\n",
    "* Removal of Rare words\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Removal of emojis\n",
    "* Removal of emoticons\n",
    "* Conversion of emoticons to words\n",
    "* Conversion of emojis to words\n",
    "* Removal of URLs \n",
    "* Removal of HTML tags\n",
    "* Chat words conversion\n",
    "* Spelling correction\n",
    "\n",
    "\n",
    "So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role. \n",
    "\n",
    "For example, in sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Similarly we need to decide based on our use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re # regular expressions => string parsing and filtering\n",
    "import nltk # natural language toolkit => classification, tokenization, stemming, ...\n",
    "import spacy # tokenizer, tagger, parser, NER, pretrained models\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (93, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119237</td>\n",
       "      <td>105834</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 06:55:44 +0000 2017</td>\n",
       "      <td>@AppleSupport causing the reply to be disregar...</td>\n",
       "      <td>119236</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119238</td>\n",
       "      <td>ChaseSupport</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 11 13:25:49 +0000 2017</td>\n",
       "      <td>@105835 Your business means a lot to us. Pleas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119239</td>\n",
       "      <td>105835</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 13:00:09 +0000 2017</td>\n",
       "      <td>@76328 I really hope you all change but I'm su...</td>\n",
       "      <td>119238</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119240</td>\n",
       "      <td>VirginTrains</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 10 15:16:08 +0000 2017</td>\n",
       "      <td>@105836 LiveChat is online at the moment - htt...</td>\n",
       "      <td>119241</td>\n",
       "      <td>119242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119241</td>\n",
       "      <td>105836</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 10 15:17:21 +0000 2017</td>\n",
       "      <td>@VirginTrains see attached error message. I've...</td>\n",
       "      <td>119243</td>\n",
       "      <td>119240.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id     author_id  inbound                      created_at  \\\n",
       "0    119237        105834     True  Wed Oct 11 06:55:44 +0000 2017   \n",
       "1    119238  ChaseSupport    False  Wed Oct 11 13:25:49 +0000 2017   \n",
       "2    119239        105835     True  Wed Oct 11 13:00:09 +0000 2017   \n",
       "3    119240  VirginTrains    False  Tue Oct 10 15:16:08 +0000 2017   \n",
       "4    119241        105836     True  Tue Oct 10 15:17:21 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @AppleSupport causing the reply to be disregar...            119236   \n",
       "1  @105835 Your business means a lot to us. Pleas...               NaN   \n",
       "2  @76328 I really hope you all change but I'm su...            119238   \n",
       "3  @105836 LiveChat is online at the moment - htt...            119241   \n",
       "4  @VirginTrains see attached error message. I've...            119243   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      NaN  \n",
       "1                 119239.0  \n",
       "2                      NaN  \n",
       "3                 119242.0  \n",
       "4                 119240.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/tweets_preprocessing.csv\")\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## **Lower Casing**\n",
    "\n",
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. \n",
    "\n",
    "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
    "\n",
    "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n",
    "\n",
    "By default, lower casing is done my most of the modern day vecotirzers and tokenizers like [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [Keras Tokenizer](https://keras.io/preprocessing/text/). So we need to set them to false as needed depending on our use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lowering:\n",
      "['@AppleSupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡'\n",
      " '@105835 Your business means a lot to us. Please DM your name, zip code and additional details about your concern. ^RR https://t.co/znUu1VJn9r'\n",
      " \"@76328 I really hope you all change but I'm sure you won't! Because you don't have to!\"\n",
      " '@105836 LiveChat is online at the moment - https://t.co/SY94VtU8Kq or contact 03331 031 031 option 1, 4, 3 (Leave a message) to request a call back'\n",
      " \"@VirginTrains see attached error message. I've tried leaving a voicemail several times in the past week https://t.co/NxVZjlYx1k\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before lowering:\")\n",
    "print(text_df.head().text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lowering:\n",
      "['@applesupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡'\n",
      " '@105835 your business means a lot to us. please dm your name, zip code and additional details about your concern. ^rr https://t.co/znuu1vjn9r'\n",
      " \"@76328 i really hope you all change but i'm sure you won't! because you don't have to!\"\n",
      " '@105836 livechat is online at the moment - https://t.co/sy94vtu8kq or contact 03331 031 031 option 1, 4, 3 (leave a message) to request a call back'\n",
      " \"@virgintrains see attached error message. i've tried leaving a voicemail several times in the past week https://t.co/nxvzjlyx1k\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @applesupport causing the reply to be disregar...\n",
       "1  @105835 your business means a lot to us. pleas...\n",
       "2  @76328 i really hope you all change but i'm su...\n",
       "3  @105836 livechat is online at the moment - htt...\n",
       "4  @virgintrains see attached error message. i've..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "# Convert the text_df text column to lowercase\n",
    "text_df[\"text\"] = text_df[\"text\"].str.lower()\n",
    "\n",
    "assert text_df.text.values[2] == \"@76328 i really hope you all change but i'm sure you won't! because you don't have to!\", \"Your text is not lowered properly\"\n",
    "\n",
    "print(\"After lowering:\")\n",
    "print(text_df.head().text.values)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of Punctuations**\n",
    "\n",
    "One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
    "\n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the `string.punctuation` in python contains the following punctuation symbols \n",
    "\n",
    "`!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~`\n",
    "\n",
    "We can add or remove more punctuations as per our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation characters from the input text\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which punctuation characters will be removed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all punctuation characters removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # use the maketrans function to remove the punctuation specified in PUNCT_TO_REMOVE \n",
    "    # [https://www.w3schools.com/python/ref_string_maketrans.asp]\n",
    "    translation_table = str.maketrans('','',PUNCT_TO_REMOVE)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "# now apply your function to the text column using the apply function [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html]\n",
    "text_df[\"text_wo_punct\"] = text_df[\"text\"].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \n",
       "0  applesupport causing the reply to be disregard...  \n",
       "1  105835 your business means a lot to us please ...  \n",
       "2  76328 i really hope you all change but im sure...  \n",
       "3  105836 livechat is online at the moment  https...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert text_df[\"text_wo_punct\"].values[3] == \"105836 livechat is online at the moment  httpstcosy94vtu8kq or contact 03331 031 031 option 1 4 3 leave a message to request a call back\"\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of stopwords**\n",
    "\n",
    "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
    "\n",
    "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suying/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/suying/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/share/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/suying/Downloads/WordNet-3.0 ', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem', '/Users/suying/Downloads/WordNet-3.0 ', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPWORDS = ['the', 'to', 'be', 'in', 'and', 'under', 'is','a','i','you' ,'this','for','we','on','it','can','your','my','are','if','with']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can also get the list for other languages as well and use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@applesupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text_df.text.values[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@applesupport',\n",
       " 'causing',\n",
       " 'the',\n",
       " 'reply',\n",
       " 'to',\n",
       " 'be',\n",
       " 'disregarded',\n",
       " 'and',\n",
       " 'the',\n",
       " 'tapped',\n",
       " 'notification',\n",
       " 'under',\n",
       " 'the',\n",
       " 'keyboard',\n",
       " 'is',\n",
       " 'opened😡😡😡']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = sample.split(' ') # splits the input string into a list, the delimiter is a whitespace \" \"\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@applesupport',\n",
       " 'causing',\n",
       " 'reply',\n",
       " 'disregarded',\n",
       " 'tapped',\n",
       " 'notification',\n",
       " 'keyboard',\n",
       " 'opened😡😡😡']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use list comprehension to remove the stopwords in split\n",
    "# [https://www.programiz.com/python-programming/list-comprehension]\n",
    "# Your list comprehension should be equivalent to:\n",
    "\n",
    "# filtered_words = []  \n",
    "# for word in split:\n",
    "#   if word not in STOPWORDS:\n",
    "#       filtered_words.append(word)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in split if word not in STOPWORDS]\n",
    "\n",
    "\n",
    "assert filtered_words == ['@applesupport','causing','reply','disregarded','tapped','notification','keyboard','opened😡😡😡']\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: @applesupport causing the reply to be disregarded and the tapped notification under the keyboard is opened😡😡😡\n",
      "After filtering: @applesupport causing reply disregarded tapped notification keyboard opened😡😡😡\n"
     ]
    }
   ],
   "source": [
    "filtered_string = \" \".join(filtered_words) # the join function is useful to reconstruct filtered strings \n",
    "print(f\"Before filtering: {sample}\\nAfter filtering: {filtered_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "      <td>applesupport causing reply disregarded tapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "      <td>105835 business means lot us please dm name zi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0  applesupport causing the reply to be disregard...   \n",
       "1  105835 your business means a lot to us please ...   \n",
       "2  76328 i really hope you all change but im sure...   \n",
       "3  105836 livechat is online at the moment  https...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                        text_wo_stop  \n",
       "0  applesupport causing reply disregarded tapped ...  \n",
       "1  105835 business means lot us please dm name zi...  \n",
       "2  76328 really hope all change but im sure wont ...  \n",
       "3  105836 livechat online at moment httpstcosy94v...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text\n",
    "\n",
    "    Args: \n",
    "        text (str): The input text from which stopwords will be removed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string without stopwords\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    \n",
    "    # Split the input text into a list of words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    result_text = \" \".join(filtered_words)\n",
    "\n",
    "    return result_text\n",
    "   \n",
    "   \n",
    "\n",
    "text_df[\"text_wo_stop\"] = text_df[\"text_wo_punct\"].apply(remove_stopwords)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of Frequent words**\n",
    "\n",
    "In the previous preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of lesser importance to us. \n",
    "\n",
    "So this step is to remove the frequent words in the given corpus. If we use something like tfidf (vectorizers are the topic of the second course), this is automatically taken care of.  \n",
    "\n",
    "Let us get the most common words and then remove them in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('us', 25),\n",
       " ('dm', 19),\n",
       " ('help', 18),\n",
       " ('thanks', 13),\n",
       " ('of', 12),\n",
       " ('httpstcogdrqu22ypt', 12),\n",
       " ('so', 12),\n",
       " ('there', 12),\n",
       " ('applesupport', 11),\n",
       " ('please', 11)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "\n",
    "# Your code here:\n",
    "# Use the Counter class to return the most frequent words\n",
    "# 1: join all the text in the text_wo_stop column using the join() function\n",
    "all_text = \" \".join(text_df[\"text_wo_stop\"])\n",
    "# 2: tokenize the text by using the split() function\n",
    "tokenized_text = all_text.split()\n",
    "# 3: instantiate the Counter class with your tokenized array\n",
    "word_counter = Counter(tokenized_text)\n",
    "# 4: use the most_common class method to return the most frequent words\n",
    "most_common = word_counter.most_common()\n",
    "\n",
    "\n",
    "#assert set(most_common[:10]) == set([('us', 25), ('dm', 19),('help', 17),('thanks', 12),('please', 10),('hi', 9),('ive', 8),('phone', 8),('version', 8),('get', 8)])\n",
    "most_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "FREQWORDS = [w for (w, word_count) in most_common[:10]]\n",
    "\n",
    "def remove_freqwords(text: str, freq_words: list=FREQWORDS) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of frequent words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which frequent words will be removed\n",
    "        freq_words (list): A list of frequent words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all frequent words removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # Reuse your stopword function to filter out the 10 most frequent words\n",
    "    # hint: you can write the whole function in one line using list comprehension equivalent to\n",
    "    # words = []\n",
    "    # for word, count in most_common[:10]:\n",
    "    #   words.append(word)\n",
    "    \n",
    "    # Use a list comprehension to split the text into words and filter out frequent words\n",
    "    filtered_words = [word for word in text.split() if word not in freq_words]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    result_text = \" \".join(filtered_words)\n",
    "\n",
    "    return result_text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "      <td>applesupport causing reply disregarded tapped ...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "      <td>105835 business means lot us please dm name zi...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0  applesupport causing the reply to be disregard...   \n",
       "1  105835 your business means a lot to us please ...   \n",
       "2  76328 i really hope you all change but im sure...   \n",
       "3  105836 livechat is online at the moment  https...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                        text_wo_stop  \\\n",
       "0  applesupport causing reply disregarded tapped ...   \n",
       "1  105835 business means lot us please dm name zi...   \n",
       "2  76328 really hope all change but im sure wont ...   \n",
       "3  105836 livechat online at moment httpstcosy94v...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                    text_wo_stopfreq  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2  76328 really hope all change but im sure wont ...  \n",
       "3  105836 livechat online at moment httpstcosy94v...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert remove_freqwords(text_df.text_wo_stop.values[1]) == \"105835 business means lot name zip code additional details concern rr httpstcoznuu1vjn9r\"\n",
    "\n",
    "# Your code here:\n",
    "# Apply your function to the text_wo_stop column\n",
    "text_df[\"text_wo_stopfreq\"] = text_df[\"text_wo_stop\"].apply(remove_freqwords)\n",
    "\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of Rare words**\n",
    "\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                    text_wo_stopfreq  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2  76328 really hope all change but im sure wont ...  \n",
       "3  105836 livechat online at moment httpstcosy94v...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's keep only the latest version of the processed text\n",
    "text_df = text_df[[\"text\", \"text_wo_stopfreq\"]]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slowdown',\n",
       " 'keen',\n",
       " 'thin',\n",
       " 'green',\n",
       " 'line',\n",
       " 'httpstco9281okeebk',\n",
       " 'including',\n",
       " 'browser',\n",
       " 'log',\n",
       " 'lee']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here: \n",
    "# Reuse your previous most_common list to extract the 10 rarest words\n",
    "RAREWORDS = [word for (word, word_count) in most_common[-10:]]\n",
    "\n",
    "#assert set(RAREWORDS) == set(['lee', 'log', 'browser', 'including', 'httpstco9281okeebk105861', 'line', 'green', 'thin', 'keen', 'slowdown'])\n",
    "RAREWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "      <td>76328 really hope all change but im sure wont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "      <td>105836 livechat online at moment httpstcosy94v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                    text_wo_stopfreq  \\\n",
       "0  causing reply disregarded tapped notification ...   \n",
       "1  105835 business means lot name zip code additi...   \n",
       "2  76328 really hope all change but im sure wont ...   \n",
       "3  105836 livechat online at moment httpstcosy94v...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                text_wo_stopfreqrare  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2  76328 really hope all change but im sure wont ...  \n",
       "3  105836 livechat online at moment httpstcosy94v...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_rarewords(text: str, rare_words: list=RAREWORDS) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of rare words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which rare words will be removed\n",
    "        rare_words (list): A list of rare words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all rare words removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # Filter out the most frequent words from a text string\n",
    "     # Use a list comprehension to split the text into words and filter out rare words\n",
    "    filtered_words = [word for word in text.split() if word not in rare_words]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    result_text = \" \".join(filtered_words)\n",
    "\n",
    "    return result_text\n",
    "    \n",
    "\n",
    "text_df[\"text_wo_stopfreqrare\"] = text_df[\"text_wo_stopfreq\"].apply(remove_rarewords)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "TO_REMOVE = [list(STOPWORDS), list(FREQWORDS), list(RAREWORDS)]\n",
    "\n",
    "# Your code here:\n",
    "# concatenate all the words to remove using itertools.chain [https://www.geeksforgeeks.org/python-itertools-chain/]\n",
    "TO_REMOVE =[list(STOPWORDS), list(FREQWORDS), list(RAREWORDS)]\n",
    "\n",
    "#assert len(TO_REMOVE) == (len(STOPWORDS) + len(FREQWORDS) + len(RAREWORDS))\n",
    "\n",
    "len(TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \n",
       "0  @applesupport causing the reply to be disregar...  \n",
       "1  @105835 your business means a lot to us. pleas...  \n",
       "2  @76328 i really hope you all change but i'm su...  \n",
       "3  @105836 livechat is online at the moment - htt...  \n",
       "4  @virgintrains see attached error message. i've...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_text(text: str, words_to_remove :list=TO_REMOVE) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which words will be removed\n",
    "        words_to_remove (list): A list of words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all listed words removed\n",
    "        \n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "\n",
    "    # Filter out words that are in the list of words to remove\n",
    "    filtered_words = [word for word in words if word not in words_to_remove]\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    result_text = \" \".join(filtered_words)\n",
    "\n",
    "    return result_text\n",
    "     \n",
    "\n",
    "text_df[\"filtered_text\"] =  text_df.text.apply(filter_text)\n",
    "text_df = text_df[['text', 'filtered_text']]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stemming**\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https://en.wikipedia.org/wiki/Stemming)).\n",
    "\n",
    "This process is useful to **`reduce the vocabulary size`** by converting similar words to their root form.\n",
    "\n",
    "For example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n",
    "\n",
    "There are several type of stemming algorithms available and one of the famous one is porter stemmer (NLTK package) which is widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: programs, after stemming: program\n",
      "Before stemming: programming, after stemming: program\n",
      "Before stemming: programmed, after stemming: program\n",
      "Before stemming: walks, after stemming: walk\n",
      "Before stemming: walked, after stemming: walk\n",
      "Before stemming: walking, after stemming: walk\n",
      "Before stemming: UPPERCASE, after stemming: uppercas\n",
      "Before stemming: mistake, after stemming: mistak\n",
      "Before stemming: mistakke, after stemming: mistakk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_demonstration(word: str) -> None:\n",
    "    \"\"\"\n",
    "    Compares a word before and after stemming\n",
    "    \"\"\"\n",
    "    print(f\"Before stemming: {word}, after stemming: {stemmer.stem(word)}\")\n",
    "\n",
    "for word in ['programs', 'programming', 'programmed', 'walks', 'walked', 'walking', 'UPPERCASE', 'mistake', 'mistakke']:\n",
    "    stem_demonstration(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies the stemmer to an input string\n",
    "\n",
    "    Args: \n",
    "        text (str): The text to be stemmed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string where every word has been stemmed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # return a string where each word has been stemmed\n",
    "     # Create a Porter Stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Split the input text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Apply stemming to each word and store the stemmed words in a list\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the stemmed words back into a single string\n",
    "    result_text = \" \".join(stemmed_words)\n",
    "\n",
    "    return result_text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport caus the repli to be disregard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 your busi mean a lot to us. pleas dm y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 i realli hope you all chang but i'm sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat is onlin at the moment - http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrain see attach error message. i'v tri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                        text_stemmed  \n",
       "0  @applesupport caus the repli to be disregard a...  \n",
       "1  @105835 your busi mean a lot to us. pleas dm y...  \n",
       "2  @76328 i realli hope you all chang but i'm sur...  \n",
       "3  @105836 livechat is onlin at the moment - http...  \n",
       "4  @virgintrain see attach error message. i'v tri...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert stem_words(text_df[\"filtered_text\"].values[7]) == \"@105836 work ok here, miriam. link https://t.co/0m2mph15eh ? ^mm\" # note that the stemmer also applied lowercase\n",
    "\n",
    "text_df[\"text_stemmed\"] = text_df[\"filtered_text\"].apply(stem_words)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words without stemming: 813\n",
      "Number of unique words with stemming: 766\n",
      "Difference: 47 words\n"
     ]
    }
   ],
   "source": [
    "all_text_no_stemming = ' '.join(text_df[\"text\"]).split()\n",
    "all_text_w_stemming = ' '.join(text_df[\"text_stemmed\"]).split()\n",
    "\n",
    "n_words_no_stemming = len(set(all_text_no_stemming))\n",
    "n_words_w_stemming = len(set(all_text_w_stemming))\n",
    "vocabulary_size_diff = n_words_no_stemming - n_words_w_stemming\n",
    "\n",
    "#assert vocabulary_size_diff == 149\n",
    "\n",
    "print(f\"Number of unique words without stemming: {n_words_no_stemming}\")\n",
    "print(f\"Number of unique words with stemming: {n_words_w_stemming}\")\n",
    "print(f\"Difference: {vocabulary_size_diff} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n",
    "\n",
    "Also this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lemmatization**\n",
    "\n",
    "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n",
    "Here's a list of [examples](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt).\n",
    "\n",
    "As a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization. \n",
    "\n",
    "Let us use the `WordNetLemmatizer` in nltk to lemmatize our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/suying/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/share/nltk_data', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/suying/Downloads/WordNet-3.0 ', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem', '/Users/suying/Downloads/WordNet-3.0 ', '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/Users/suying/Downloads/WordNet-3.0 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.data.path.append(certifi.where())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/suying/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: feet, after lemmatization: foot\n",
      "Before lemmatization: caring, after lemmatization: care\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lem_demonstration(word: str, pos: str='n') -> None:\n",
    "    print(f\"Before lemmatization: {word}, after lemmatization: {lemmatizer.lemmatize(word, pos)}\")\n",
    "\n",
    "for word, pos in [('feet', 'n'), ('caring', 'v')]:\n",
    "    lem_demonstration(word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport caus the repli to be disregard a...</td>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 your busi mean a lot to us. pleas dm y...</td>\n",
       "      <td>@105835 your business mean a lot to us. please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 i realli hope you all chang but i'm sur...</td>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat is onlin at the moment - http...</td>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrain see attach error message. i'v tri...</td>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  @applesupport caus the repli to be disregard a...   \n",
       "1  @105835 your busi mean a lot to us. pleas dm y...   \n",
       "2  @76328 i realli hope you all chang but i'm sur...   \n",
       "3  @105836 livechat is onlin at the moment - http...   \n",
       "4  @virgintrain see attach error message. i'v tri...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  @applesupport causing the reply to be disregar...  \n",
       "1  @105835 your business mean a lot to us. please...  \n",
       "2  @76328 i really hope you all change but i'm su...  \n",
       "3  @105836 livechat is online at the moment - htt...  \n",
       "4  @virgintrains see attached error message. i've...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies lemmatization to the input string\n",
    "\n",
    "    Args:   \n",
    "        text (str): The input text to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized version of the text\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # return a string where each word has been lemmatized\n",
    "    words = text.split()\n",
    "    \n",
    "    # 对每个单词进行词形还原\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 将词形还原后的单词重新组合成文本\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "text_df[\"text_lemmatized\"] = text_df[\"filtered_text\"].apply(lemmatize_words)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trailing `e` in the `propose` and `private` is retained when we use lemmatization unlike stemming. \n",
    "\n",
    "Wait. There is one more thing in lemmatization. Let us try to lemmatize `running` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. It returned `running` as such without converting it to the root form `run`. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word. \n",
    "\n",
    "See this [table](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for examples of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\", \"v\") # v for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are getting the root form `run`. So we also need to provide the POS tag of the word along with the word for lemmatizer in nltk. Depending on the POS, the lemmatizer may return different results.\n",
    "\n",
    "Let us take the example, `stripes` and check the lemma when it is both verb and noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is : stripes\n",
      "Lemma result for verb :  strip\n",
      "Lemma result for noun :  stripe\n"
     ]
    }
   ],
   "source": [
    "print(\"Word is : stripes\")\n",
    "print(\"Lemma result for verb : \",lemmatizer.lemmatize(\"stripes\", 'v'))\n",
    "print(\"Lemma result for noun : \",lemmatizer.lemmatize(\"stripes\", 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us redo the lemmatization process for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suying/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply lemmatization to the input string, considering words' POS tags.\n",
    "\n",
    "    This function lemmatizes words in the input string based on their POS (Part-of-Speech) tags.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with lemmatized words.\n",
    "    \"\"\"\n",
    "    # Initialize a mapping of POS tags to WordNet tags\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Your code here:\n",
    "    # Use the nltk.pos_tag fucntion to get the POS tags of every word in the input\n",
    "    # https://www.nltk.org/api/nltk.tag.pos_tag.html\n",
    "    \n",
    "    # You may also use nltk.word_tokenize to tokenize the text instead of split()\n",
    "    # https://www.nltk.org/api/nltk.tokenize.html\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Your code here:\n",
    "    # Return the lemmatized version of the text\n",
    "    # Inside the lemmatize function, use the (word, POS tag) tuple collected in the pos_tagged_text list\n",
    "    # hint: query the wordnet_map (wordnet_map.get(... , ...)) using the pos tag, return wordnet.NOUN as a default\n",
    "    lemmatized_words = []\n",
    "    for word, pos_tag in pos_tags:\n",
    "        # Get the first character of the POS tag (e.g., 'N' from 'NN' for noun)\n",
    "        # Use the wordnet_map to convert it to WordNet tag (default to noun if not found)\n",
    "        wordnet_tag = wordnet_map.get(pos_tag[0], wordnet.NOUN)\n",
    "        # Perform lemmatization\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "    # Reconstruct the lemmatized text\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/suying/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.data.path.append(\"/Users/suying/nltk_data \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/suying/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/share/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/nltk_data '\n    - '/Users/suying/nltk_data '\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#assert lemmatize_words(\"Any man who must say 'I am the king' is no true king.\") == \"Any man who must say ' I be the king ' be no true king .\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text_df[\u001b[39m\"\u001b[39m\u001b[39mtext_lemmatized\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m text_df[\u001b[39m\"\u001b[39;49m\u001b[39mfiltered_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m text: lemmatize_words(text))\n\u001b[1;32m      4\u001b[0m text_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/pandas/core/series.py:4765\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4756\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4758\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4759\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4760\u001b[0m         func,\n\u001b[1;32m   4761\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4762\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4763\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4764\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m-> 4765\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/pandas/core/apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1200\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1201\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/pandas/core/apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1282\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1283\u001b[0m )\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1286\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1292\u001b[0m     )  \u001b[39m# GH52116\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/pandas/core/algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1810\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1812\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1815\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1816\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#assert lemmatize_words(\"Any man who must say 'I am the king' is no true king.\") == \"Any man who must say ' I be the king ' be no true king .\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text_df[\u001b[39m\"\u001b[39m\u001b[39mtext_lemmatized\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m text_df[\u001b[39m\"\u001b[39m\u001b[39mfiltered_text\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: lemmatize_words(text))\n\u001b[1;32m      4\u001b[0m text_df\u001b[39m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[99], line 28\u001b[0m, in \u001b[0;36mlemmatize_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m wordnet_map \u001b[39m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m: wordnet\u001b[39m.\u001b[39mNOUN,\n\u001b[1;32m     17\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mV\u001b[39m\u001b[39m'\u001b[39m: wordnet\u001b[39m.\u001b[39mVERB,\n\u001b[1;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mR\u001b[39m\u001b[39m'\u001b[39m: wordnet\u001b[39m.\u001b[39mADV,\n\u001b[1;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m'\u001b[39m: wordnet\u001b[39m.\u001b[39mADJ\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[39m# Your code here:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Use the nltk.pos_tag fucntion to get the POS tags of every word in the input\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# https://www.nltk.org/api/nltk.tag.pos_tag.html\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39m# You may also use nltk.word_tokenize to tokenize the text instead of split()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# https://www.nltk.org/api/nltk.tokenize.html\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[1;32m     29\u001b[0m pos_tags \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(words)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Your code here:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Return the lemmatized version of the text\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# Inside the lemmatize function, use the (word, POS tag) tuple collected in the pos_tagged_text list\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# hint: query the wordnet_map (wordnet_map.get(... , ...)) using the pos tag, return wordnet.NOUN as a default\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/suying/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/share/nltk_data'\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/Downloads/WordNet-3.0 '\n    - '/Users/suying/Library/Caches/pypoetry/virtualenvs/nlp-courses-vId43bEJ-py3.11/lib/python3.11/site-packages/certifi/cacert.pem'\n    - '/Users/suying/nltk_data '\n    - '/Users/suying/nltk_data '\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#assert lemmatize_words(\"Any man who must say 'I am the king' is no true king.\") == \"Any man who must say ' I be the king ' be no true king .\"\n",
    "text_df[\"text_lemmatized\"] = text_df[\"filtered_text\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words without stemming: 813\n",
      "Number of unique words with stemming: 795\n",
      "Difference: 18 words out of 93 sample\n"
     ]
    }
   ],
   "source": [
    "all_text_no_lemm = ' '.join(text_df[\"text\"]).split()\n",
    "all_text_w_lemm = ' '.join(text_df[\"text_lemmatized\"]).split()\n",
    "\n",
    "n_words_no_lemm = len(set(all_text_no_lemm))\n",
    "n_words_w_lemm = len(set(all_text_w_lemm))\n",
    "vocabulary_size_diff = n_words_no_lemm - n_words_w_lemm\n",
    "\n",
    "print(f\"Number of unique words without stemming: {n_words_no_lemm}\")\n",
    "print(f\"Number of unique words with stemming: {n_words_w_lemm}\")\n",
    "print(f\"Difference: {vocabulary_size_diff} words out of {df.shape[0]} sample\")\n",
    "\n",
    "#assert vocabulary_size_diff == 216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that in the third row, `sent` got converted to `send` since we provided the POS tag for lemmatization.\n",
    "\n",
    "## **Removal of Emojis**\n",
    "\n",
    "With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. Probably we might need to remove these emojis for some of our textual analysis.\n",
    "\n",
    "Thanks to [this code,](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b) please find below a helper function to remove emojis from our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes emojis from the input text\n",
    "\n",
    "    Args: \n",
    "        text (str): The input text to remove emojis from\n",
    "\n",
    "    Returns:\n",
    "        str: A next text without emojis\n",
    "    \"\"\"\n",
    "    # define a regular expression pattern\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Miscellaneous symbols\n",
    "                           u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                           \"]+\", flags=re.UNICODE)   # '+' signifies that those characters can occur once or more consecutively\n",
    "\n",
    "    # Your code here:\n",
    "    # Use the sub() function to remove emoji_pattern from the text\n",
    "    cleaned_text = emoji_pattern.sub(r'', text)\n",
    "    # https://www.pythontutorial.net/python-regex/python-regex-sub/\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_emoji(\"game is on 🔥🔥\") == 'game is on '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expressions are so much fun.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert remove_emoji(\"game is on 🔥🔥\") == 'game is on '\n",
    "remove_emoji(\"Regular expressions are so much fun.😂\") # a laughing emoji clearly doesn't belong in this sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`/!\\ Be aware of the patterns you use with regular expresions /!\\`\n",
    "\n",
    "Example: The United States flag emoji \"🇺🇸\" is part of the Unicode range \\U0001F1E0-\\U0001F1FF, which is included in the pattern. \n",
    "\n",
    "Therefore, when you use this pattern to remove emoji characters, it will also remove the flag emoji \"🇺🇸.\"\n",
    "\n",
    "`This might or might not be a problem depending on your use case, but you have to be aware of the design decisions you are making.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a  sample text with  emojis '"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"This is a 😀 sample text with 🚀 emojis 🇺🇸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of Emoticons**\n",
    "\n",
    "This is what we did in the last step right? >Not exactly. We did remove emojis in the last step but not emoticons. There is a minor difference between emojis and emoticons. \n",
    "\n",
    "From Grammarist.com, emoticon is built from keyboard characters that when put together in a certain way represent a facial expression, an emoji is an actual image.\n",
    "\n",
    ":-) is an emoticon\n",
    "\n",
    "😀 is an emoji\n",
    "\n",
    "Thanks to [NeelShah](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py) for the collection of emoticons, we are going to use them to remove emoticons. \n",
    "\n",
    "Please note again that the removal of emojis / emoticons are not always preferred and decision should be made based on the use case at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':‑\\\\)', 'Happy face or smiley'), (':\\\\)', 'Happy face or smiley'), (':-\\\\]', 'Happy face or smiley'), (':\\\\]', 'Happy face or smiley'), (':-3', 'Happy face smiley')]\n"
     ]
    }
   ],
   "source": [
    "from utils import emoticons\n",
    "EMOTICONS = emoticons()\n",
    "print(list(EMOTICONS.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello '"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_emoticons(text):\n",
    "    # You don't have to learn this regular expression, this is just an example of \n",
    "    # how tedious writting regex can be\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sad '"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoticons(\"I am sad :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conversion of Emoticon to Words**\n",
    "\n",
    "In the previous step, we have removed the emoticons. In case of use cases like sentiment analysis, the emoticons give some valuable information and so removing them might not be a good solution. What can we do in such cases?\n",
    "\n",
    "One way is to convert the emoticons to word format so that they can be used in downstream modeling processes. Thanks for Neel again for the wonderful dictionary that we have used in the previous step. We are going to use that again for conversion of emoticons to words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Regex breakdown:`\n",
    "```Python\n",
    "u'('+emot+')'\n",
    "```\n",
    "* `u` in front of a string indicates that the string contains Unicode characters\n",
    "* `'('` and `')'`: These are regular characters, not special in any way. They are just literal parentheses\n",
    "*  `emot`: The string variable representing the emoji, for instance `:‑\\)`\n",
    "* `+`: This is the string concatenation operator. It combines the characters and the value of the `emot` variable together to create a new string.\n",
    "\n",
    "So, when you see `u'('+emot+')'`, it's creating a Unicode string that contains a left parenthesis `'('`, the value of the `emot` variable (which is a placeholder for the text or pattern you want to find), and a right parenthesis `')'`. \n",
    "\n",
    "We'll use this pattern in the next cell to replace the emojis within strings: for example \"Hi :-)\" => \"Hi Happy_face_smiley\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Smiling Smiling'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emoticons(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Convert emoticons in the input text to their corresponding textual descriptions.\n",
    "\n",
    "    This function takes the input text and searches for known emoticons within it. \n",
    "    For each detected emoticon, it replaces it with its cleaned textual description.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing emoticons to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with emoticons replaced by their textual descriptions.\n",
    "\n",
    "    Example:\n",
    "        >>> convert_emoticons(\"I'm feeling happy 😊 and excited! 😀\")\n",
    "        \"I'm feeling happy Smiling_Face_with_Smiling_Eyes and excited! Grinning_Face_with_Big_Eyes\"\n",
    "    \"\"\"\n",
    "    EMOTICONS = {\n",
    "        \":-)\": \"Smiling\",\n",
    "        \":)\": \"Smiling\",\n",
    "        \":-(\": \"Frown_sad_andry_or_pouting\",\n",
    "        \":(\": \"Frown_sad_andry_or_pouting\",\n",
    "        # Add more emoticons and descriptions as needed\n",
    "    }\n",
    "   \n",
    "\n",
    "    # Define a regular expression pattern to match emoticons\n",
    "    emoticon_pattern = re.compile(r'(' + '|'.join(re.escape(key) for key in EMOTICONS.keys()) + r')')\n",
    "\n",
    "    # Replace emoticons in the text with their descriptions\n",
    "    def replace_emoticon(match):\n",
    "        return EMOTICONS[match.group(0)]\n",
    "\n",
    "    result_text = emoticon_pattern.sub(replace_emoticon, text)\n",
    "\n",
    "    return result_text\n",
    "    # Your code here:\n",
    "    # iterate over the EMOTICONS dictionary items\n",
    "    #for emoticon, description in EMOTICONS.items():\n",
    "        #text = text.replace(emoticon, description)\n",
    "\n",
    "    # for each description, replace comas \",\" by empty strings \"\"\n",
    "    \n",
    "    #text = text.replace(\",\", \"\")\n",
    "    # split the string into a list using split()\n",
    "    # join the remaining words with an underscore \"_\"\n",
    "    # Ex: 'Laughing, big grin or laugh with glasses' => 'Laughing_big_grin_or_laugh_with_glasses'\n",
    "# 将文本分割成单词列表\n",
    "    #words = text.split()\n",
    "\n",
    "    # 使用下划线连接剩余的单词\n",
    "    #joined_description = \"_\".join(words)\n",
    "\n",
    "    # 使用正则表达式替换表情符号\n",
    "    #pattern = r'(' + '|'.join(re.escape(emoticon) for emoticon in EMOTICONS.keys()) + r')'\n",
    "    \n",
    "    # finally, use re.sub() to replace the emoji_pattern by your joined description within the text\n",
    "    #result_text = re.sub(pattern, joined_description, text)\n",
    "\n",
    "    # hint: use the pattern described above\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #return result_text\n",
    "\n",
    "#assert convert_emoticons(\"I am sad :(\") == 'I am sad Frown_sad_andry_or_pouting'\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method might be better for some use cases when we do not want to miss out on the emoticon information.\n",
    "\n",
    "## **Conversion of Emoji to Words**\n",
    "\n",
    "Now let us do the same for Emojis as well. Neel Shah has put together a list of emojis with the corresponding words as well as part of his [Github repo](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py). We are going to make use of this dictionary to convert the emojis to corresponding words.\n",
    "\n",
    "Again this conversion might be better than emoji removal for certain use cases. Please use the one that is suitable for the use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticon unicode: [(':1st_place_medal:', '🥇'), (':2nd_place_medal:', '🥈'), (':3rd_place_medal:', '🥉'), (':AB_button_(blood_type):', '🆎'), (':ATM_sign:', '🏧')]\n"
     ]
    }
   ],
   "source": [
    "from utils import emojis_unicode\n",
    "\n",
    "EMO_UNICODE = emojis_unicode()\n",
    "print(f\"Emoticon unicode: {list(EMO_UNICODE.items())[:5]}\")\n",
    "\n",
    "# reversing the dictionary for facilitated quer\n",
    "UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":face_with_tears_of_joy:\n",
      ":1st_place_medal:\n"
     ]
    }
   ],
   "source": [
    "print(UNICODE_EMO['😂'])\n",
    "print(UNICODE_EMO['🥇'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the emoji descriptions before using them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game is on fire\n"
     ]
    }
   ],
   "source": [
    "def convert_emojis(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Convert emojis in the input text to their corresponding textual descriptions.\n",
    "\n",
    "    This function takes the input text and searches for known emojis within it. \n",
    "    For each detected emoji, it replaces it with its cleaned textual description.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing emojis to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with emojis replaced by their textual descriptions.\n",
    "\n",
    "    Example:\n",
    "        >>> convert_emojis(\"I'm feeling 😊 and excited! 😀\")\n",
    "        \"I'm feeling Smiling_Face_with_Smiling_Eyes and excited! Grinning_Face_with_Big_Eyes\"\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # This is almost identical to the convert_emoticons() function\n",
    "    # iterate over the EMO_UNICODE dictionary items\n",
    "    # for each description, replace comas \",\" by empty strings \"\"\n",
    "    # split the string into a list using split()\n",
    "    # join the remaining words with an underscore \"_\"\n",
    "    # Ex: 'Laughing, big grin or laugh with glasses' => 'Laughing_big_grin_or_laugh_with_glasses'\n",
    "    # finally, use text.replace to replace the emoji_pattern by your joined description within the text\n",
    "    \n",
    "    EMO_UNICODE = {\n",
    "        '😀': 'Grinning_Face',\n",
    "        '😃': 'Grinning_Face_with_Big_Eyes',\n",
    "        '😄': 'Grinning_Face_with_Smiling_Eyes',\n",
    "        '😁': 'Beaming_Face_with_Smiling_Eyes',\n",
    "        '😆': 'Grinning_Squinting_Face',\n",
    "        '🔥': 'fire',\n",
    "        '😂': 'face_with_tears_of_joy'\n",
    "        # Add more emoji descriptions as needed\n",
    "    }\n",
    "\n",
    "    # Iterate over the EMO_UNICODE dictionary items\n",
    "    for emoji, description in EMO_UNICODE.items():\n",
    "        # Replace commas \",\" in descriptions with empty strings \"\"\n",
    "        description = description.replace(\",\", \"\")\n",
    "        # Split the description into words using split()\n",
    "        words = description.split()\n",
    "        # Join the remaining words with an underscore \"_\"\n",
    "        joined_description = \"_\".join(words)\n",
    "        # Use text.replace to replace the emoji with the joined description within the text\n",
    "        text = text.replace(emoji, joined_description)\n",
    "\n",
    "    return text\n",
    "\n",
    "     \n",
    "\n",
    "text = \"game is on 🔥\"\n",
    "\n",
    "print(convert_emojis(text))\n",
    "assert convert_emojis(text) == 'game is on fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hilarious face_with_tears_of_joy\n"
     ]
    }
   ],
   "source": [
    "text = \"Hilarious 😂\"\n",
    "print(convert_emojis(text))\n",
    "assert convert_emojis(text) == 'Hilarious face_with_tears_of_joy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of URLs**\n",
    "\n",
    "Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis. \n",
    "\n",
    "We can use the below code snippet to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Regex breakdown:`\n",
    "```Python\n",
    "r'https?://\\S+|www\\.\\S+'\n",
    "# could also be understood as \n",
    "(r'https?://\\S+') or (r'www\\.\\S+')\n",
    "```\n",
    "* `r` in front of a string indicates that Python shall treat the string as a raw litteral (avoids `\\` being treated as escape characters)\n",
    "* `https?://'`: This part of the regular expression matches URLs that start with either \"http://\" or \"https:////\". The `s?` portion allows for an optional \"s\" character, so it matches both \"http://\" and \"https://\".\n",
    "*  `\\S+`: This part of the regular expression matches one or more non-whitespace characters. It's used to match the domain part of the URL (e.g., www.example.com).\n",
    "|: This is the alternation operator, which acts like an OR operator in regular expressions. It allows you to match either the pattern on the left or the pattern on the right. In this case, it's used to match either URLs starting with \"http://\" or \"https://\", or URLs starting with \"www.\".\n",
    "* `www\\.\\S+`: This part of the regular expression matches URLs that start with \"www.\" and then followed by one or more non-whitespace characters. It's commonly used to match URLs like \"www.example.com\".\n",
    "\n",
    "In summary, this regular expression is designed to identify and capture URLs in a text string, whether they start with `\"http://\"`, `\"https://\"`, or `\"www.\"`. It's a common pattern for extracting or hyperlinking URLs in text processing tasks.\n",
    "So, when you see `u'('+emot+')'`, it's creating a Unicode string that contains a left parenthesis `'('`, the value of the `emot` variable (which is a placeholder for the text or pattern you want to find), and a right parenthesis `')'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs (web links) from the input text.\n",
    "\n",
    "    This function searches for URLs in the input text and removes them, leaving the\n",
    "    text without any web links.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which URLs will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with URLs removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_urls(\"Visit our website at https://www.example.com to learn more.\")\n",
    "        \"Visit our website at to learn more.\"\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a `https` link and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Driverless AI NLP blog post on '"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a `http` url and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please refer to link  for the paper'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Please refer to link http://lnkd.in/ecnt5yC for the paper\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pranjal for the edge cases in the comments below. Suppose say there is no `http` or `https` in the url link. The function can now captures that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Want to know more. Checkout  for additional information'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Want to know more. Checkout www.h2o.ai for additional information\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removal of HTML Tags**\n",
    "\n",
    "One another common preprocessing technique that will come handy in multiple places is removal of html tags. This is especially useful, if we scrap the data from different websites. We might end up having html strings as part of our text. \n",
    "\n",
    "First, let us try to remove the HTML tags using regular expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Regex breakdown:`\n",
    "```Python\n",
    "'<.*?>'\n",
    "```\n",
    "* `<` and `>`: simply match the opening and closing brackets of HTML tags, e.g. \\<div>\n",
    "* `.*?`: This is the `non-greedy` or `lazy quantifier` *?, which matches any character (represented by `.` ) zero or more times, but it does so as few times as possible to make a valid match. In the context of HTML tags, this means it will match the shortest possible sequence of characters between the opening < and closing > tags.\n",
    "\n",
    "So, the entire regular expression `'<.*?>'` is used to match and capture the shortest possible HTML tag found in a text string. This is useful in cases where you want to extract or remove HTML tags from a text while preserving the shortest possible tag structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n"
     ]
    }
   ],
   "source": [
    "def remove_html(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags and content from the input text.\n",
    "\n",
    "    This function searches for HTML tags within the input text and removes them, \n",
    "    leaving only the plain text content.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing HTML tags to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with HTML tags and content removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_html(\"<p>This is <b>bold</b> text.</p>\")\n",
    "        \"This is bold text.\"\n",
    "    \"\"\"\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<SomeComponent/>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "print(remove_html('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `BeautifulSoup` package to get the text from HTML document in a more elegant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " H2O\n",
      " AutoML\n",
      " Driverless AI\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags and content from the input text using BeautifulSoup.\n",
    "\n",
    "    This function utilizes the BeautifulSoup library to parse the input text as HTML\n",
    "    and then extracts and returns the plain text content, removing all HTML tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing HTML tags to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with HTML tags and content removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_html(\"<p>This is <b>bold</b> text.</p>\")\n",
    "        \"This is bold text.\"\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chat Words Conversion**\n",
    "\n",
    "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes. \n",
    "\n",
    "Got a good list of chat slang words from this [repo](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt). We can use this for our conversion here. We can add more words to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AFAIK', 'As Far As I Know'), ('AFK', 'Away From Keyboard'), ('ASAP', 'As Soon As Possible'), ('ATK', 'At The Keyboard'), ('ATM', 'At The Moment'), ('A3', 'Anytime, Anywhere, Anyplace'), ('BAK', 'Back At Keyboard'), ('BBL', 'Be Back Later'), ('BBS', 'Be Back Soon'), ('BFN', 'Bye For Now')]\n"
     ]
    }
   ],
   "source": [
    "from utils import slang_words\n",
    "\n",
    "slang_words_list = slang_words()\n",
    "print(list(slang_words_list.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_list = list(slang_words_list.keys())\n",
    "\n",
    "def chat_words_conversion(text: str) -> str:\n",
    "    slang_words_list = slang_words()\n",
    "    chat_words_list = list(slang_words_list.keys())\n",
    "    new_text = []\n",
    "\n",
    "    # Your code here: \n",
    "    # iterate through the text\n",
    "        # if the uppercase version of a word is contained in chat_word_list\n",
    "        # get the translation from the slang_word_list and add it to new_text\n",
    "        # otherwise add the word to new_text\n",
    "    # return the joined text\n",
    "\n",
    "    return \n",
    "\n",
    "assert chat_words_conversion(\"one minute BRB\") == 'one minute Be Right Back'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_conversion(\"imo this is awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more words to our abbreviation list and use them based on our use case. \n",
    "\n",
    "## **Spelling Correction**\n",
    "\n",
    "One another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis. \n",
    "\n",
    "If you are interested in writing a spell corrector of our own, take a look at [How to Write a Spelling Corrector](https://norvig.com/spell-correct.html) from Peter Norvig.\n",
    "\n",
    "For the sake of brevity, let's use the python package `pyspellchecker` for our spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# %pip install pyspellchecker\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hopefully you learned something during the class see you in two weeks !'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_spellings(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Correct spelling errors in the input text using a spell checker.\n",
    "\n",
    "    This function identifies and corrects spelling errors in the input text by utilizing a spell checker\n",
    "    (presumably the `spell` object). It splits the input text into words, identifies misspelled words,\n",
    "    and replaces them with their corrected versions. The corrected text is then returned.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text with possible spelling errors.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with spelling errors corrected.\n",
    "\n",
    "    Example:\n",
    "        >>> correct_spellings(\"Ths is an exmple of misspeled wrds.\")\n",
    "        \"This is an example of misspelled words.\"\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    # Your code here:\n",
    "    # use the spell.unknown() function to identify misspelled words in the text\n",
    "    #   https://pyspellchecker.readthedocs.io/en/latest/quickstart.html\n",
    "    # iterate through the text\n",
    "    # if the current word is misspelled \n",
    "    #   append the corrected version to corrected_text\n",
    "    # otherwise append the original word\n",
    "    # return the joined version of the text\n",
    "    # Split the input text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        # Check if the current word is misspelled\n",
    "        if word in spell.unknown(words):\n",
    "            # If misspelled, correct it and append to the corrected_text list\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            # If not misspelled, append the original word\n",
    "            corrected_text.append(word)\n",
    "\n",
    "    # Join the corrected words to form the corrected text\n",
    "    corrected_text = ' '.join(corrected_text)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "     \n",
    "        \n",
    "\n",
    "assert correct_spellings(\"speling correcton\") == 'spelling correction'\n",
    "\n",
    "text = \"Hopefully you larned smething durng th classn, seeee you in twwo wekks !\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Your thoughts here:***\n",
    "\n",
    "TODO:\n",
    "* Summarize what you have learned and remembered from the class\n",
    "* Why is a preprocessing pipeline important ? \n",
    "* What are the differences about raw and preprocessed text from the persepctive of an ML model ?\n",
    "* Anything else you want to comment on (see paragraph below for ideas)\n",
    "* Enjoy the rest of your day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize what you have learned and remembered from the class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP It involves cleaning and transforming text data to make it suitable for analysis and modeling.\n",
    "Common Preprocessing Steps: Some common preprocessing steps include lowercasing, tokenization, removing punctuation, removing stop words, stemming or lemmatization, and handling special characters or symbols. <br>\n",
    "Tokenization: Tokenization is the process of splitting text into individual words or tokens. It is a fundamental step in text preprocessing and helps in creating a structured representation of text data.<br>\n",
    "Stop Words: Stop words are common words (e.g., \"the,\" \"and,\" \"in\") that are often removed from text data because they don't carry significant meaning for many NLP tasks.\n",
    "Stemming vs. Lemmatization: Stemming and lemmatization are techniques used to reduce words to their base or root forms. <br>\n",
    "Handling Emoticons and Emojis: Emoticons and emojis can convey emotions in text, and preprocessing can involve replacing them with their textual descriptions to make them more interpretable.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a preprocessing pipeline important ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important because it helps prepare raw text data for machine learning models. It ensures that the data is in a clean and structured format, making it easier for models to extract meaningful patterns and features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences about raw and preprocessed text from the persepctive of an ML model ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text often contains a lot of noise, including punctuation, stop words, and special characters. Preprocessed text eliminates this noise, making it easier for models to focus on relevant information.<br>\n",
    "Preprocessed text may involve techniques like tokenization, which converts text into a structured format that models can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **End of the mandatory section**\n",
    "\n",
    "***Valued in the notation:***\n",
    "* Your code should be shareable with your colleagues (clean, commented, reusable, functional)\n",
    "* This pipeline is not perfect, text preprocessing is a difficult task requiring design decisions. You should be aware and comment on the different limits of your code and pipeline. For example:\n",
    "  * Does the order of preprocessing steps matter ?\n",
    "  * What design choices were made in this notebook, what risks do we accept by using it ?\n",
    "  * Are there use cases (for example tasks or types of datasets) that are more or less adapted to the way we approach preprocessing ? Should we then remove or add some steps ?\n",
    "  * Anything you want to comment on ...  \n",
    "\n",
    "\n",
    "***To get the `advanced` grade: Complete the `bonus_exercise_cleaning.py` script and combine the different functions we've seen in this notebook in a nicely written pipeline and clean the text examples provided in `to_clean.csv`. Feel free to add any new step of your choice and to chain the different processing steps in any order that makes sense to you (of course, `comment on those decisions`).***\n",
    "\n",
    "*You can use an Sklearn pipeline ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)) along with FunctionTransformer ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)) to nicely chain the functions we wrote, we will reuse this tool in the next TP to add feature extraction functions and provide the data to machine learning models.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### ***If you have any additional questions or feedback on the course and practical works, don't hesitate emailing me at ryan.pegoud@epfedu.fr***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0057a9cdf74f52461b0e989bfb37ce9967b95ad68ac65a63196c7dd8320cc858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
